{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357989df-bd87-408a-99fb-e165117136f2",
   "metadata": {},
   "source": [
    "## **Download this notebook**\n",
    "Download this notebook by pressing on `File` > `Download`\n",
    "\n",
    "## **Install & import relevant packages**\n",
    "\n",
    "To build a simple chatbot using Python, we need to install the following packages. Each package serves a specific purpose in our chatbot pipeline:\n",
    "\n",
    "- **`ollama`**: This package allows us to run large language models (LLMs) locally. It simplifies interactions with models like LLaMA and Mistral.\n",
    "- **`langchain`**: A framework for building applications powered by LLMs. It provides tools for chaining prompts, managing memory, and integrating models.\n",
    "- **`chromadb`**: A vector database used for storing and retrieving text embeddings. This is essential for making the chatbot context-aware.\n",
    "- **`gradio`**: A simple way to build web-based interfaces for machine learning models. We’ll use it to create a user-friendly chatbot interface.\n",
    "- **`langchain-community`**: A collection of integrations and utilities that extend `langchain`, making it easier to work with external tools and databases.\n",
    "- **`pymupdf`**: To work with PDF documents, we need to install `pymupdf` which makes it easy to handle PDF files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b56b22d-880c-4dc1-8325-3aea0f31511b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install ollama\n",
    "# !pip install langchain chromadb gradio \n",
    "# !pip install -U langchain-community\n",
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09504989-e4da-443c-98c0-db67fc15f22f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import ollama  # Enables interaction with local large language models (LLMs)\n",
    "import gradio as gr  # Provides an easy-to-use web interface for the chatbot\n",
    "\n",
    "# Document processing and retrieval  \n",
    "from langchain_community.document_loaders import PyMuPDFLoader  # Extracts text from PDF files for processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits text into smaller chunks for better embedding and retrieval\n",
    "from langchain.vectorstores import Chroma  # Handles storage and retrieval of vector embeddings using ChromaDB\n",
    "\n",
    "# Embedding generation  \n",
    "from langchain_community.embeddings import OllamaEmbeddings  # Converts text into numerical vectors using Ollama's embedding model\n",
    "\n",
    "import re  # Provides tools for working with regular expressions, useful for text cleaning and pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13b5d5-15d3-463d-b794-6980a1f329f7",
   "metadata": {},
   "source": [
    "## **Call DeepSeek R1 1.5B via API**\n",
    "\n",
    "In this snippet, we use `ollama.chat()` to generate a response from DeepSeek R1 1.5B (which is installed locally). Let’s break it down:\n",
    "\n",
    "- **Choosing the Model**: We specify `\"deepseek-r1:1.5b\"` using the `model` argument.\n",
    "- **Passing User Messages**: The `messages` parameter is a list of interactions, where each message contains:\n",
    "  - `\"role\": \"user\"` – Indicates that the message is from the user.\n",
    "  - `\"content\": \"Explain Newton's second law of motion\"` – The actual question asked.\n",
    "- **Extracting and Printing the Response**: The model generates a structured response, where the content of the reply is stored in `response[\"message\"][\"content\"]`. We print this output to display the answer.\n",
    "\n",
    "This approach allows us to interact with an LLM locally, making it a powerful way to answer queries without relying on external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea826cef-9dba-4441-bf66-f50098955fe1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton's Second Law of Motion, also known as the \"Law of Acceleration,\" is a fundamental concept in physics that describes how an object responds to force. It states:\n",
      "\n",
      "**\"The acceleration of an object is directly proportional to the force applied and inversely proportional to its mass.\"**\n",
      "\n",
      "Mathematically, this law can be expressed as:\n",
      "\n",
      "**F = ma**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **F** is the net force acting on an object (measured in Newtons)\n",
      "* **m** is the mass of the object (measured in kilograms)\n",
      "* **a** is the acceleration of the object (measured in meters per second squared)\n",
      "\n",
      "This means that if you apply a greater force to an object, it will accelerate more quickly. Conversely, if you increase the mass of the object, it will require more force to achieve the same level of acceleration.\n",
      "\n",
      "Here are some key implications of Newton's Second Law:\n",
      "\n",
      "1. **The relationship between force and acceleration is direct**: When you double the force applied to an object, its acceleration will also double.\n",
      "2. **Mass affects acceleration**: The more massive an object is, the less it accelerates in response to a given force.\n",
      "3. **Weight vs. mass**: Weight (the force of gravity acting on an object) is often confused with mass, but they are distinct concepts. Mass remains constant, while weight changes depending on location.\n",
      "\n",
      "Some common examples that illustrate Newton's Second Law include:\n",
      "\n",
      "* A car accelerating quickly from 0 to 60 km/h when the driver presses the accelerator\n",
      "* A bowling ball rolling down a lane and gaining speed due to gravity\n",
      "* A child pushing a toy cart across the floor with varying degrees of force\n",
      "\n",
      "Remember, Newton's Second Law is a fundamental concept that has far-reaching implications in many areas of science and engineering.\n"
     ]
    }
   ],
   "source": [
    "# Call the Ollama model to generate a response  \n",
    "response = ollama.chat(\n",
    "    model=\"llama3.1\",  # Specifies the DeepSeek R1 model (1.5B parameters)\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain Newton's second law of motion\"},  # User's input query\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(response[\"message\"][\"content\"])  # Extracts and displays the generated response from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c69a03-fee0-4198-8d55-e68a0f650ad6",
   "metadata": {},
   "source": [
    "## Preprocess the PDF Document for RAG\n",
    "\n",
    "We will now create a function that pre-processes the PDF file for RAG. Below is a breakdown of its logic:\n",
    "\n",
    "- **Check if a PDF is provided**: If no file is uploaded, the function returns `None`, preventing unnecessary processing.\n",
    "- **Extract text from the PDF**: Uses `PyMuPDFLoader` to load and extract raw text from the document.\n",
    "- **Split the text into chunks**: Since LLMs process smaller text fragments better, we use `RecursiveCharacterTextSplitter`. Each chunk contains **500 characters**, with an **overlap of 100 characters** to maintain context.\n",
    "- **Generate embeddings for each chunk**: Uses `OllamaEmbeddings` with the `\"deepseek-r1:1.5b\"` model to convert text into **numerical vectors**. These embeddings allow us to find **meaning-based matches** rather than exact keyword searches.\n",
    "- **Store embeddings in a vector database**: We use `ChromaDB` to **store and organize** the generated embeddings efficiently. The data is **persisted** in `\"./chroma_db\"` to avoid recomputing embeddings every time.\n",
    "- **Create a retriever for searching the database**: The retriever acts like a **smart search engine**, enabling the chatbot to fetch the most relevant text when answering questions.\n",
    "- **Return essential components**\n",
    "    - `text_splitter` (for future text processing)\n",
    "    - `vectorstore` (holding the document embeddings)\n",
    "    - `retriever` (allowing AI-powered search over the document)\n",
    "\n",
    "## **What are embeddings?**\n",
    "Embeddings are **numerical representations of text** that capture meaning. Instead of treating words as just sequences of letters, embeddings transform them into **multi-dimensional vectors** where similar words or sentences **are placed closer together**.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/1*OEmWDt4eztOcm5pr2QbxfA.png)\n",
    "_Source: https://medium.com/towards-data-science/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba_\n",
    "\n",
    "### **Intuition: how do embeddings work?**\n",
    "Imagine a **map of words**:\n",
    "- Words with **similar meanings** (*cat* and *dog*) are **closer together**.\n",
    "- Words with **different meanings** (*cat* and *car*) are **farther apart**.\n",
    "- Sentences or paragraphs with similar **context** will have embeddings that are **close to each other**.\n",
    "\n",
    "This means when a user asks a question, the LLM doesn’t just look for **exact words**—it finds the **most relevant text based on meaning**, even if the wording is different.\n",
    "\n",
    "### **Why this matters?**\n",
    "This function enables a chatbot to **understand and retrieve information from PDFs efficiently**. Instead of simple keyword searches, it **finds contextually relevant information**, making AI responses **more accurate and useful**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d2b0f7-a3ec-418f-9807-20b6ddba252d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the function that processes the PDF\n",
    "def process_pdf(pdf_bytes):\n",
    "    # If PDF files are empty, return None — This prevents errors from trying to process an empty input.\n",
    "    if pdf_bytes is None:\n",
    "        return None, None, None\n",
    "    # PyMuPDFLoader initializes the PDF file\n",
    "    loader = PyMuPDFLoader(pdf_bytes) \n",
    "    # .load() method reads the content of the PDF and extracts its text\n",
    "    data = loader.load()\n",
    "    # RecursiveCharacterTextSplitter splits the PDF into chunks of 500 characters, keeping 100 characters overlap to keep context \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    # Splits the documents into chunks and stores them in chunks object\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    # Create embeddings using OllamaEmbeddings \n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.1\")\n",
    "    # Create a vector database which allows us to store the chunks and their embeddings\n",
    "    vectorstore=Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"./chroma_db\")  # Example directory\n",
    "    # This creates a retriever that enables searching through the vectorstore.\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \"\"\"\n",
    "    The function returns 3 objects\n",
    "        text_splitter → (Used to split new text in the same way as before)\n",
    "        vectorstore → (Holds the processed document chunks)\n",
    "        retriever → (Used to fetch relevant document chunks when answering questions)\n",
    "    \"\"\"\n",
    "    \n",
    "    return text_splitter, vectorstore, retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec4a86-8335-4f52-bbee-2a2c8085fcaa",
   "metadata": {},
   "source": [
    "## **Combining retrieved document chunks**\n",
    "Once the embeddings are retrieved, next we need to stitch these together. The `combine_docs() function merges multiple retrieved document chunks into a single string. Why do we do this?\n",
    "\n",
    "- **Provides better context** – LLMs understand structured, continuous text better than fragmented pieces.  \n",
    "- **Improves response quality** – Merging chunks helps LLMs generate more coherent and complete answers.  \n",
    "- **Preserves document flow** – Keeps information logically ordered, preventing disjointed responses.  \n",
    "- **Optimizes token usage** – Reduces redundant queries and ensures efficient use of the model’s context window.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdce7c97-8c46-402f-a574-42139ad67509",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33982b85-946a-42e5-80f5-3369ffb9df1e",
   "metadata": {},
   "source": [
    "## Querying DeepSeek-R1 using Ollama\n",
    "\n",
    "Now, our input to the model is ready. Let’s set up DeepSeek R1 using Ollama.\n",
    "\n",
    "The `ollama_llm()` function **takes a user’s question and relevant context, formats a structured prompt, sends it to the DeepSeek-R1 model, and returns a clean generated response**.\n",
    "\n",
    "### **How it works (step-by-step)**\n",
    "- **Formats the input** – Structures the question and context for better input understanding.\n",
    "- **Calls `deepseek-r1`** – Sends the formatted prompt to generate a response.\n",
    "- **Extracts the response content** – Retrieves the AI’s answer.\n",
    "- **Cleans unnecessary text** – Removes `<think>...</think>` traces that contain model reasoning.\n",
    "- **Returns the cleaned response** – Provides a polished and readable AI answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5d31ca-42ba-4c0e-8f7b-5233e776dc98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ollama_llm(question, context):\n",
    "\n",
    "    # Format the prompt with the question and context to provide structured input for the AI\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    # Send the structured prompt to the Ollama model for processing\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.1\",  # Specifies the AI model to use\n",
    "        messages=[{'role': 'user', 'content': formatted_prompt}]  # Formats the user input\n",
    "    )\n",
    "    # Extract the AI-generated response content\n",
    "    response_content = response['message']['content']\n",
    "    # Remove content inside <think>...</think> tags to clean up AI reasoning traces\n",
    "    final_answer = re.sub(r'<think>.*?</think>', # We're searching for think tags\n",
    "                          '', # We'll replace them with empty spaces\n",
    "                          response_content, # In response_content\n",
    "                          flags=re.DOTALL).strip() # (dot) should match newlines (\\n) as well.\n",
    "    # Return the final cleaned response\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106285be-8517-4744-bae6-81722e6e0d5c",
   "metadata": {},
   "source": [
    "## **Build a RAG pipeline** \n",
    "\n",
    "Now we have all the required components, let’s build the RAG pipeline for our demo. We will build the `rag_chain()` function, which **retrieves relevant document chunks, formats them, and generates a response with the additional context from the retrieval step**. \n",
    "\n",
    "### **How it works**\n",
    "\n",
    "- **Retrieves relevant document chunks**: The `retriever.invoke(question)` searches for the most relevant text based on the user's question.Instead of relying solely on a language model’s memory, it **fetches factual data** from stored documents.\n",
    "- **Formats the retrieved content**: `combine_docs(retrieved_docs)` merges the document chunks into a single structured text. This ensures that DeepSeek receives a **well-organized input** for better reasoning.\n",
    "- **Generates the response**: Calls `ollama_llm(question, formatted_content)`, which:  \n",
    "    - Passes the structured input to `deepseek-r1:1.5b` for processing.  \n",
    "    - Cleans up the response (removes `<think>` tags).  \n",
    "    - Returns a polished, fact-based answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecf97fc9-c2a1-45c0-b716-56c5a9d30708",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define rag_chain function for Retrieval Augmented Generation\n",
    "def rag_chain(question, text_splitter, vectorstore, retriever):\n",
    "    \"\"\"\n",
    "    This function takes as input:\n",
    "        - The question we want to ask the model\n",
    "        - The text_splitter object to split the PDF and read into chunks\n",
    "        - The vectorstore for retrieving embeddings \n",
    "        - The retriever objects which retrieves data from the vectorstore\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(question) # In this step, we will find the part of the document most relevant to the question\n",
    "    formatted_content = combine_docs(retrieved_docs) # We will then combine the retrieved parts of the document \n",
    "    return ollama_llm(question, formatted_content) # Run the model on the question, and the relevant context from the document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e6a285-8920-4717-a33d-972c3e54e19d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Put it all together — Create a function that performs the logic expected by the Chatbot  \n",
    "def ask_question(pdf_bytes, question): \n",
    "    text_splitter, vectorstore, retriever = process_pdf(pdf_bytes) # Process the PDF\n",
    "    if text_splitter is None:\n",
    "        return None  # No PDF uploaded    \n",
    "    result = rag_chain(question, text_splitter, vectorstore, retriever) # Return the results with RAG\n",
    "    return {result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e8fcf-1d5f-4c5c-9bc2-9a14550eee1c",
   "metadata": {},
   "source": [
    "## **Building a Chat Interface with Gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbbdfcc2-466b-4190-8711-b9ebce57b1b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2164666/580498603.py:15: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"llama3.1\")\n"
     ]
    }
   ],
   "source": [
    "# Define a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ask_question,  # The function that processes user input and generates a response (logic of the app)\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF (optional)\"),  # Optional file upload input for a PDF document\n",
    "        gr.Textbox(label=\"Ask a question\")  # Text input where the user types their question\n",
    "    ],\n",
    "    outputs=\"text\",  # The function returns a text response\n",
    "    title=\"Ask questions about your PDF\",  # The title displayed on the interface\n",
    "    description=\"Use DeepSeek-R1 1.5B to answer your questions about the uploaded PDF document.\",  # Brief description of the interface's functionality\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface to start the web-based app\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f0953-6a97-469a-825c-22161f386acb",
   "metadata": {},
   "source": [
    "## **How to uninstall Ollama and DeepSeek?**\n",
    "\n",
    "**To uninstall Ollama**\n",
    "\n",
    "To uninstall Ollama\n",
    "\n",
    "- macOS: Delete Ollama from applications\n",
    "- Windows: Uninstall Ollama using control panel\n",
    "\n",
    "**To uninstall DeepSeek**\n",
    "\n",
    "Search for these folders on your computer — delete these folders. \n",
    "\n",
    "- macOS: ~/.ollama/models\n",
    "- Windows: C:\\Users\\%username%\\.ollama\\models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
