# Import required classes from LangChain and LangChain-Ollama
from langchain_ollama import OllamaLLM  # Interface to communicate with a local Ollama LLM server
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate  # To create prompt templates
from langchain_core.output_parsers import StrOutputParser  # To parse model outputs into strings
from langchain_core.runnables import RunnablePassthrough  # A passthrough node used in LCEL chains

# Model Initialization 
# Create an instance of the LLaMA model
# - Specify the model name ("llama3.1")
# - Provide the base URL where the Ollama server is running
llama = OllamaLLM(model="llama3.1", base_url="http://localhost:11434")


# Without Using LCEL (Direct Prompting) 
#######################################
# Define an **unstructured prompt template** for a simple question
# This will later be formatted with a specific country name
template = "What is the capital of {country}?"
prompt_template = ChatPromptTemplate.from_template(template)  # Create a ChatPromptTemplate from the raw template string

# Format the prompt by substituting the {country} variable
# Note: We use .format() to fill in "France" dynamically
prompt = prompt_template.format(country="France")

# Send the formatted prompt directly to the LLM and get a response
result = llama.invoke(prompt)

# Print the model's output
print("No LCEL: ", result)


# Using LCEL (LangChain Expression Language) 
###########################################

# Define a structured PromptTemplate using LCEL conventions
# This template again defines a question about a country's capital
prompt = PromptTemplate.from_template("What is capital of {country}?")

# Define an LCEL **Chain** using operators (|) to link multiple components
# Hereâ€™s how the chain works:
# - RunnablePassthrough(): Initially passes the input without modifying it
# - prompt: Formats the input into a structured prompt
# - llama: Sends the formatted prompt to the LLM for a response
# - StrOutputParser(): Extracts and formats the final output as a clean string
chain = (
    RunnablePassthrough()
    | prompt
    | llama
    | StrOutputParser()
)

# Execute the chain by providing an input dictionary
# LCEL automatically maps the "country" key to the {country} placeholder
response = chain.invoke({"country": "France"})

# Print the output generated by the model using the LCEL chain
print("Using LCEL: ", response)

