# Install Required Packages
# Run this manually if needed:
# pip install requests ollama langchain chromadb gradio langchain-community pymupdf

# Import Required Libraries
import requests  # For HTTP requests to the Ollama server
import json  # For handling JSON responses
import re  # For text cleanup (regex to remove unwanted tags)
import gradio as gr  # For building the web UI
from langchain_community.document_loaders import PyMuPDFLoader  # To load PDFs into text
from langchain.text_splitter import RecursiveCharacterTextSplitter  # To split large text into smaller chunks
from langchain_community.vectorstores import Chroma  # Vector store (ChromaDB) for retrieval
from langchain_ollama import OllamaEmbeddings  # For creating embeddings using the LLaMA model

# Define the Ollama Server URL
OLLAMA_API_URL = "http://localhost:11434/api/generate"  # Local Ollama API endpoint


# Define Function: Stream LLM Responses
def ollama_llm_stream(question, context):
    """
    Streams the model's response token-by-token while sending the full prompt 
    (question + retrieved context) to the Ollama server.
    """
    # Format the prompt to include both the question and supporting context
    formatted_prompt = f"Question: {question}\n\nContext: {context}"

    # Define the request payload
    payload = {
        "model": "llama3.1",
        "prompt": formatted_prompt,
        "stream": True  # Enable streaming mode
    }

    try:
        # Send streaming POST request to Ollama
        with requests.post(OLLAMA_API_URL, json=payload, stream=True) as response:
            response.raise_for_status()  # Raise error if bad response

            full_output = ""  # To accumulate the output as it streams
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode("utf-8"))  # Parse each JSON chunk
                    delta = chunk.get("response", "")  # Extract response field
                    full_output += delta  # Accumulate partial output

                    # Clean up any <think> tags if present
                    cleaned_output = re.sub(r'<think>.*?</think>', '', full_output, flags=re.DOTALL).strip()

                    # Yield the current cleaned output incrementally
                    yield cleaned_output
    except requests.RequestException as e:
        # If API request fails, yield the error message
        yield f"Error contacting Ollama API: {str(e)}"


# Define Function: Process Uploaded PDF
def process_pdf(pdf_path):
    """
    Loads and splits a PDF into manageable text chunks,
    then creates a vector store (ChromaDB) with embeddings.
    """
    if pdf_path is None:
        return None, None, None

    # Load PDF using PyMuPDF
    loader = PyMuPDFLoader(pdf_path)
    data = loader.load()

    # Split text into overlapping chunks for better retrieval
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = text_splitter.split_documents(data)

    # Create vectorstore using embeddings generated by LLaMA model
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    #vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory="/home/prabir/ollama-book/APX-1-VectorDB/chroma_db")
    vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory="/home/prabir/matt-videoprojects/2024-04-04-build-rag-with-python/my_chroma_data")

    # Convert vectorstore into retriever interface
    retriever = vectorstore.as_retriever()

    return text_splitter, vectorstore, retriever


# Define Helper Function: Combine Retrieved Docs
def combine_docs(docs):
    """
    Combines multiple document chunks into a single string.
    """
    return "\n\n".join(doc.page_content for doc in docs)


# Define the Full RAG Chain with Streaming
def rag_chain_stream(question, text_splitter, vectorstore, retriever):
    """
    Retrieves relevant chunks and streams a generated answer based on them.
    """
    retrieved_docs = retriever.invoke(question)  # Retrieve relevant chunks based on the question
    formatted_content = combine_docs(retrieved_docs)  # Combine retrieved text chunks
    return ollama_llm_stream(question, formatted_content)  # Pass to LLM stream


# Define the Wrapper Function for Gradio Streaming Interface
def ask_question_stream(pdf_file, question):
    """
    Wrapper function used by Gradio UI.
    - Processes uploaded PDF
    - Runs retrieval and generation
    - Streams answer back to UI
    """
    text_splitter, vectorstore, retriever = process_pdf(pdf_file.name)

    if text_splitter is None:
        yield "Please upload a PDF file."
        return

    # Stream the answer chunk-by-chunk
    for chunk in rag_chain_stream(question, text_splitter, vectorstore, retriever):
        yield chunk


# Build the Gradio Web UI
demo = gr.Interface(
    fn=ask_question_stream,  # Main function to call
    inputs=[
        #gr.File(label="Upload PDF"),  # Upload widget for the PDF
        gr.Textbox(label="Ask a question")  # Textbox for the user's question
    ],
    outputs=gr.Textbox(label="Response", lines=10),  # Streaming output textbox
    title="Ask Questions About Medicare 2025 (Streaming) + Git",  # App title
    description="Uses LLaMA 3.1 (served by Ollama) to answer your questions about Medicare 2025 + Git. Streaming output enabled.",  # Short app description
    flagging_mode="never"  # Disable Gradio's flagging feature
)


# Launch the Gradio App
if __name__ == '__main__':
    # Launch the Gradio app and optionally make it public (share=True)
    demo.launch(share=True)

